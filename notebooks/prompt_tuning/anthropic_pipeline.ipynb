{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0bd1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb0402",
   "metadata": {},
   "source": [
    "# State and Output Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f97e7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhannguyen/Documents/HCMUS/LLM/Code/llm-resume-evaluation/.env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic model to structure the LLM's output\n",
    "class ScorePrediction(BaseModel):\n",
    "    \"\"\"The structured output for the resume score.\"\"\"\n",
    "\n",
    "    score: float = Field(description=\"The matching score between JD and Resume (0â€“10)\")\n",
    "    explanation: str = Field(description=\"The explanation of the matching score\")\n",
    "\n",
    "\n",
    "# The state that will be passed through the graph\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"The state of our graph.\"\"\"\n",
    "\n",
    "    jd: str\n",
    "    resume: str\n",
    "    prediction: ScorePrediction  # The final output will be stored here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f857d58",
   "metadata": {},
   "source": [
    "# Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef7698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# The complete, optimized prompt generated by DSPy\n",
    "OPTIMIZED_PROMPT_TEMPLATE = \"\"\"\n",
    "# SYSTEM INSTRUCTION\n",
    "Your input fields are:\n",
    "1. `jd` (str): The job description text\n",
    "2. `resume` (str): The candidate's resume text\n",
    "Your output fields are:\n",
    "1. `score` (str): The matching score between JD and Resume (0â€“10)\n",
    "2. `explanation` (str): The explanation of the matching score\n",
    "All interactions will be structured in the following way, with the appropriate values filled in.\n",
    "\n",
    "[[ ## jd ## ]]\n",
    "{{jd}}\n",
    "\n",
    "[[ ## resume ## ]]\n",
    "{{resume}}\n",
    "\n",
    "[[ ## score ## ]]\n",
    "{{score}}\n",
    "\n",
    "[[ ## explanation ## ]]\n",
    "{{explanation}}\n",
    "\n",
    "[[ ## completed ## ]]\n",
    "In adhering to this structure, your objective is: \n",
    "        Given a job description and a resume, predict a matching score (0-10).\n",
    "\n",
    "# FEW-SHOT EXAMPLE 1\n",
    "[[ ## jd ## ]]\n",
    "Software Engineer needed with experience in Git, Docker, Java, REST APIs. Hit summer discussion culture measure ever.\n",
    "\n",
    "[[ ## resume ## ]]\n",
    "Experienced professional skilled in System Design, past, Java, foot. Thank case rather generation inside. Raise new structure race.\n",
    "\n",
    "[[ ## score ## ]]\n",
    "4\n",
    "\n",
    "[[ ## explanation ## ]]\n",
    "Not supplied for this particular example.\n",
    "\n",
    "[[ ## completed ## ]]\n",
    "\n",
    "# FEW-SHOT EXAMPLE 2\n",
    "[[ ## jd ## ]]\n",
    "ML Engineer needed with experience in Python, PyTorch, Computer Vision, TensorFlow, Keras. Could chair beautiful social both few through. Entire card much rate politics their identify. Pass sing goal during be those.\n",
    "\n",
    "[[ ## resume ## ]]\n",
    "Experienced professional skilled in Computer Vision, Python, MLOps, PyTorch, TensorFlow, parent. Exactly section network detail. Short out team author deal hospital able.\n",
    "\n",
    "[[ ## score ## ]]\n",
    "10\n",
    "\n",
    "[[ ## explanation ## ]]\n",
    "Not supplied for this particular example.\n",
    "\n",
    "[[ ## completed ## ]]\n",
    "\n",
    "# FEW-SHOT EXAMPLE 3\n",
    "[[ ## jd ## ]]\n",
    "ML Engineer needed with experience in TensorFlow, Computer Vision, Keras, PyTorch, MLOps. Approach wish fine near. Agree long behind stuff how positive tree. Quality team general office painting official.\n",
    "\n",
    "[[ ## resume ## ]]\n",
    "Experienced professional skilled in Cloud, interest, MLOps, Keras, hotel, PyTorch. Address guy fund window well impact quite. Place raise really feeling vote per. Catch board present market society fight foreign. Out generation beyond six degree stop.\n",
    "\n",
    "[[ ## score ## ]]\n",
    "8\n",
    "\n",
    "[[ ## explanation ## ]]\n",
    "Not supplied for this particular example.\n",
    "\n",
    "[[ ## completed ## ]]\n",
    "\n",
    "# FEW-SHOT EXAMPLE 4\n",
    "[[ ## jd ## ]]\n",
    "Data Scientist needed with experience in Deep Learning, Machine Learning, NLP, SQL, Pandas, Python, Statistics. Nearly computer close garden. Law individual business hair show. Final though kitchen purpose five. Use sea right civil.\n",
    "\n",
    "[[ ## resume ## ]]\n",
    "Experienced professional skilled in Python, Deep Learning, garden, Statistics, SQL, Pandas, stock. Note operation despite born. Step take share million message long board.\n",
    "\n",
    "[[ ## score ## ]]\n",
    "8\n",
    "\n",
    "[[ ## explanation ## ]]\n",
    "Not supplied for this particular example.\n",
    "\n",
    "[[ ## completed ## ]]\n",
    "\n",
    "# FINAL QUERY\n",
    "[[ ## jd ## ]]\n",
    "{jd}\n",
    "\n",
    "[[ ## resume ## ]]\n",
    "{resume}\n",
    "\n",
    "Respond with the corresponding output fields, starting with the field `[[ ## score ## ]]`, then `[[ ## explanation ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def scorer_node(state: GraphState):\n",
    "    \"\"\"Invokes the LLM to score the resume against the job description.\"\"\"\n",
    "\n",
    "    # Ensure your OPENAI_API_KEY is set in your environment\n",
    "    model = ChatAnthropic(model=\"claude-3-7-sonnet-20250219\", temperature=0)\n",
    "\n",
    "    # Create the prompt from the template\n",
    "    prompt = ChatPromptTemplate.from_template(OPTIMIZED_PROMPT_TEMPLATE)\n",
    "\n",
    "    # Create the chain\n",
    "    chain = prompt | model\n",
    "\n",
    "    # Get inputs from the state\n",
    "    jd = state[\"jd\"]\n",
    "    resume = state[\"resume\"]\n",
    "\n",
    "    # Invoke the chain\n",
    "    response = chain.invoke({\"jd\": jd, \"resume\": resume})\n",
    "    response_text = response.content\n",
    "\n",
    "    # Parse the structured output from the model's response\n",
    "    try:\n",
    "        score_match = re.search(r\"\\[\\[ ## score ## \\]\\]\\s*([\\d.]+)\", response_text)\n",
    "        explanation_match = re.search(\n",
    "            r\"\\[\\[ ## explanation ## \\]\\]\\s*([\\s\\S]*?)\\s*\\[\\[ ## completed ## \\]\\]\",\n",
    "            response_text,\n",
    "        )\n",
    "\n",
    "        if not score_match or not explanation_match:\n",
    "            raise ValueError(\"Could not find score or explanation tags in the output.\")\n",
    "\n",
    "        score_val = float(score_match.group(1).strip())\n",
    "        explanation_val = explanation_match.group(1).strip()\n",
    "\n",
    "        prediction = ScorePrediction(score=score_val, explanation=explanation_val)\n",
    "\n",
    "    except (ValueError, AttributeError) as e:\n",
    "        print(f\"Error parsing LLM output: {e}\\nResponse: {response_text}\")\n",
    "        # Assign a default low score on parsing failure\n",
    "        prediction = ScorePrediction(\n",
    "            score=0.0, explanation=\"Failed to parse model output.\"\n",
    "        )\n",
    "\n",
    "    return {\"prediction\": prediction}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23af82b5",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7cc68fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… LangGraph App Compiled Successfully!\n",
      "\n",
      "--- Example Result ---\n",
      "Score: 6.0\n",
      "Explanation: The resume shows moderate alignment with the job description for a Product Manager position. Here's the analysis:\n",
      "\n",
      "Matching skills:\n",
      "- Stakeholder Management: Directly mentioned in both the JD and resume\n",
      "- Agile: Directly mentioned in both the JD and resume\n",
      "- Scrum: While not explicitly mentioned in the JD, Scrum is closely related to Agile methodology\n",
      "\n",
      "Missing skills:\n",
      "- User Stories: A key requirement in the JD but not mentioned in the resume\n",
      "- Product Roadmap: An important PM skill in the JD but not present in the resume\n",
      "\n",
      "The resume also contains some irrelevant or unclear terms like \"blood\", \"skill\", and \"quality\" which don't directly relate to product management responsibilities.\n",
      "\n",
      "The candidate demonstrates knowledge of Agile methodologies and stakeholder management which are important for the role, but lacks explicit experience with core product management functions like creating user stories and managing product roadmaps. This suggests they may have worked in an Agile environment, possibly in an adjacent role, but might not have comprehensive product management experience.\n",
      "\n",
      "A score of 6/10 reflects that the candidate meets some key requirements but is missing critical product management skills specified in the job description.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# Initialize the graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add the scorer node\n",
    "workflow.add_node(\"scorer\", scorer_node)\n",
    "\n",
    "# Set the entry point and the final edge\n",
    "workflow.set_entry_point(\"scorer\")\n",
    "workflow.add_edge(\"scorer\", END)\n",
    "\n",
    "# Compile the graph into a runnable application\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"\\nâœ… LangGraph App Compiled Successfully!\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "jd_example = \"Product Manager needed with experience in User Stories, Product Roadmap, Stakeholder Management, Agile.\"\n",
    "resume_example = \"Experienced professional skilled in blood, Scrum, skill, Stakeholder Management, quality, Agile.\"\n",
    "\n",
    "inputs = {\"jd\": jd_example, \"resume\": resume_example}\n",
    "\n",
    "# Run the graph\n",
    "result = app.invoke(inputs)\n",
    "\n",
    "print(\"\\n--- Example Result ---\")\n",
    "print(f\"Score: {result['prediction'].score}\")\n",
    "print(f\"Explanation: {result['prediction'].explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a43152",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74457447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "def accuracy_at_threshold(gold_scores, pred_scores, threshold):\n",
    "    gold_scores = np.array(gold_scores)\n",
    "    pred_scores = np.array(pred_scores)\n",
    "    absolute_errors = np.abs(gold_scores - pred_scores)\n",
    "    within_threshold_count = np.sum(absolute_errors <= threshold)\n",
    "    return within_threshold_count / len(gold_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a96d4c",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d863337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"../../data/train.csv\")\n",
    "val_df = pd.read_csv(\"../../data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ccf63e",
   "metadata": {},
   "source": [
    "## Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d302e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Evaluation on 3000 examples ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LangGraph:   0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LangGraph:  22%|â–ˆâ–ˆâ–       | 668/3000 [1:13:55<4:11:25,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during graph execution: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'This request would exceed the rate limit for your organization (b28eb3d3-0d52-4991-aa6d-35c473076012) of 1,000,000 input tokens per minute. For details, refer to: https://docs.claude.com/en/api/rate-limits. You can see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}, 'request_id': 'req_011CU7VuvHSm68XVtiqUfRVq'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LangGraph: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [5:30:54<00:00,  6.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Complete ---\n",
      "\n",
      "--- LangGraph Evaluation Results ---\n",
      "ðŸ“Š Mean Absolute Error (MAE): 0.8713\n",
      "---\n",
      "ðŸŽ¯ Accuracy@1 (error <= 1.0): 89.43%\n",
      "ðŸŽ¯ Accuracy@2 (error <= 2.0): 97.33%\n",
      "ðŸŽ¯ Accuracy@3 (error <= 3.0): 99.90%\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "gold_scores = []\n",
    "pred_scores = []\n",
    "\n",
    "print(f\"\\n--- Starting Evaluation on {len(val_df)} examples ---\")\n",
    "\n",
    "for i, example in tqdm(\n",
    "    val_df.iterrows(), desc=\"Evaluating LangGraph\", total=len(val_df)\n",
    "):\n",
    "    # Get ground truth data\n",
    "    jd = example[\"job_description\"]\n",
    "    resume = example[\"resume\"]\n",
    "    gold_score = example[\"match_score\"]\n",
    "\n",
    "    # Run the LangGraph app\n",
    "    inputs = {\"jd\": jd, \"resume\": resume}\n",
    "    try:\n",
    "        result = app.invoke(inputs)\n",
    "        pred_score = result[\"prediction\"].score\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during graph execution: {e}\")\n",
    "        pred_score = 0.0  # Assign a penalty score\n",
    "\n",
    "    gold_scores.append(gold_score)\n",
    "    pred_scores.append(pred_score)\n",
    "\n",
    "    # To avoid rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"--- Evaluation Complete ---\")\n",
    "\n",
    "# --- Calculate Metrics ---\n",
    "\n",
    "# 1. Mean Absolute Error (MAE)\n",
    "final_mae = mean_absolute_error(gold_scores, pred_scores)\n",
    "\n",
    "# 2. Accuracy@threshold\n",
    "acc_at_1 = accuracy_at_threshold(gold_scores, pred_scores, threshold=1)\n",
    "acc_at_2 = accuracy_at_threshold(gold_scores, pred_scores, threshold=2)\n",
    "acc_at_3 = accuracy_at_threshold(gold_scores, pred_scores, threshold=3)\n",
    "\n",
    "\n",
    "# --- Print Results ---\n",
    "\n",
    "print(\"\\n--- LangGraph Evaluation Results ---\")\n",
    "print(f\"ðŸ“Š Mean Absolute Error (MAE): {final_mae:.4f}\")\n",
    "print(\"---\")\n",
    "print(f\"ðŸŽ¯ Accuracy@1 (error <= 1.0): {acc_at_1:.2%}\")\n",
    "print(f\"ðŸŽ¯ Accuracy@2 (error <= 2.0): {acc_at_2:.2%}\")\n",
    "print(f\"ðŸŽ¯ Accuracy@3 (error <= 3.0): {acc_at_3:.2%}\")\n",
    "print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0f4ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"preds/preds_anthropic_kaggle.json\", \"w\") as f:\n",
    "    json.dump(pred_scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486bdf5",
   "metadata": {},
   "source": [
    "## Collected dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9705152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LangGraph: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87/87 [14:19<00:00,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Complete ---\n",
      "\n",
      "--- LangGraph Evaluation Results ---\n",
      "ðŸ“Š Mean Absolute Error (MAE): 2.5517\n",
      "---\n",
      "ðŸŽ¯ Accuracy@1 (error <= 1.0): 36.78%\n",
      "ðŸŽ¯ Accuracy@2 (error <= 2.0): 55.17%\n",
      "ðŸŽ¯ Accuracy@3 (error <= 3.0): 68.97%\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "collected_folder = \"/Users/nhannguyen/ngtuthanhan@gmail.com - Google Drive/My Drive/HCMUS/LLM/Data_Collector_30_8\"\n",
    "matching_json = os.path.join(collected_folder, \"Matching_Result.json\")\n",
    "\n",
    "with open(matching_json, \"r\") as f:\n",
    "    matching_data = json.load(f)\n",
    "\n",
    "pred_scores = []\n",
    "gold_scores = []\n",
    "\n",
    "for example in tqdm(\n",
    "    matching_data, desc=\"Evaluating LangGraph\", total=len(matching_data)\n",
    "):\n",
    "    jd_path = example[\"JD\"]\n",
    "    resume_path = example[\"CV\"]\n",
    "    gold_score = example[\"Score\"]\n",
    "    with open(os.path.join(collected_folder, \"JD\", jd_path) + \".txt\", \"r\") as f:\n",
    "        jd = f.read()\n",
    "    with open(os.path.join(collected_folder, \"CV\", resume_path) + \".txt\", \"r\") as f:\n",
    "        resume = f.read()\n",
    "    inputs = {\"jd\": jd, \"resume\": resume}\n",
    "    try:\n",
    "        result = app.invoke(inputs)\n",
    "        pred_score = result[\"prediction\"].score\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during graph execution: {e}\")\n",
    "        pred_score = 0.0  # Assign a penalty score\n",
    "\n",
    "    gold_scores.append(gold_score)\n",
    "    pred_scores.append(pred_score)\n",
    "\n",
    "    # To avoid rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "print(\"--- Evaluation Complete ---\")\n",
    "\n",
    "# --- Calculate Metrics ---\n",
    "\n",
    "# 1. Mean Absolute Error (MAE)\n",
    "final_mae = mean_absolute_error(gold_scores, pred_scores)\n",
    "\n",
    "# 2. Accuracy@threshold\n",
    "acc_at_1 = accuracy_at_threshold(gold_scores, pred_scores, threshold=1)\n",
    "acc_at_2 = accuracy_at_threshold(gold_scores, pred_scores, threshold=2)\n",
    "acc_at_3 = accuracy_at_threshold(gold_scores, pred_scores, threshold=3)\n",
    "\n",
    "\n",
    "# --- Print Results ---\n",
    "\n",
    "print(\"\\n--- LangGraph Evaluation Results ---\")\n",
    "print(f\"ðŸ“Š Mean Absolute Error (MAE): {final_mae:.4f}\")\n",
    "print(\"---\")\n",
    "print(f\"ðŸŽ¯ Accuracy@1 (error <= 1.0): {acc_at_1:.2%}\")\n",
    "print(f\"ðŸŽ¯ Accuracy@2 (error <= 2.0): {acc_at_2:.2%}\")\n",
    "print(f\"ðŸŽ¯ Accuracy@3 (error <= 3.0): {acc_at_3:.2%}\")\n",
    "print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8defc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"preds/preds_anthropic_collected.json\", \"w\") as f:\n",
    "    json.dump(pred_scores, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
