{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54caf1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18acc7f",
   "metadata": {},
   "source": [
    "# Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2980570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "\n",
    "class JDResumeScoring(dspy.Signature):\n",
    "    \"\"\"Given a job description and a resume, predict a matching score (0-10).\"\"\"\n",
    "\n",
    "    jd = dspy.InputField(desc=\"The job description text\")\n",
    "    resume = dspy.InputField(desc=\"The candidate's resume text\")\n",
    "    score = dspy.OutputField(desc=\"The matching score between JD and Resume (0–10)\")\n",
    "    explanation = dspy.OutputField(desc=\"The explanation of the matching score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0336d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = dspy.LM(\n",
    "    model=\"openai/gpt-4.1-mini-2025-04-14\",\n",
    ")\n",
    "dspy.configure(lm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694f300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JDResumeMatcher(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scorer = dspy.Predict(JDResumeScoring)\n",
    "\n",
    "    def forward(self, jd, resume):\n",
    "        return self.scorer(jd=jd, resume=resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c90d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "def mae_metric(gold, pred, trace=None):\n",
    "    try:\n",
    "        pred_score = float(pred.score)\n",
    "    except Exception:\n",
    "        return 9999\n",
    "    return mean_absolute_error([gold.score], [pred_score])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b0fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def accuracy_at_threshold(gold_scores, pred_scores, threshold):\n",
    "    \"\"\"\n",
    "    Calculates the Accuracy@threshold metric.\n",
    "\n",
    "    The formula is: (1/N) * Σ 1(|Score_GT - Score_Pred| <= threshold)\n",
    "    where 1() is the indicator function.\n",
    "    \"\"\"\n",
    "    gold_scores = np.array(gold_scores)\n",
    "    pred_scores = np.array(pred_scores)\n",
    "    absolute_errors = np.abs(gold_scores - pred_scores)\n",
    "    within_threshold_count = np.sum(absolute_errors <= threshold)\n",
    "    return within_threshold_count / len(gold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4b49c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"../../data/train.csv\")\n",
    "val_df = pd.read_csv(\"../../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f42f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "394ac0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/11 13:28:49 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "RUNNING WITH THE FOLLOWING LIGHT AUTO RUN SETTINGS:\n",
      "num_trials: 10\n",
      "minibatch: True\n",
      "num_fewshot_candidates: 6\n",
      "num_instruct_candidates: 3\n",
      "valset size: 100\n",
      "\n",
      "2025/10/11 13:28:49 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/10/11 13:28:49 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/10/11 13:28:49 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=6 sets of demonstrations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 1/6\n",
      "Bootstrapping set 2/6\n",
      "Bootstrapping set 3/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/6000 [00:09<3:18:58,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples for up to 1 rounds, amounting to 5 attempts.\n",
      "Bootstrapping set 4/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/6000 [00:05<3:04:42,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n",
      "Bootstrapping set 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/6000 [00:01<2:00:07,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
      "Bootstrapping set 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/6000 [00:03<2:32:51,  1.53s/it]\n",
      "2025/10/11 13:29:09 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "2025/10/11 13:29:09 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n",
      "2025/10/11 13:29:09 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "Proposing N=3 instructions...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/11 13:29:34 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
      "\n",
      "2025/10/11 13:29:34 INFO dspy.teleprompt.mipro_optimizer_v2: 0: Given a job description and a resume, predict a matching score (0-10).\n",
      "\n",
      "2025/10/11 13:29:34 INFO dspy.teleprompt.mipro_optimizer_v2: 1: Given a job description and a candidate's resume, analyze the degree of alignment between the two by focusing on matching domain-specific and technical skills while disregarding irrelevant or nonsensical filler text. Produce a matching score from 0 to 10 that reflects how well the candidate’s qualifications meet the job requirements. Additionally, provide a detailed explanation highlighting which specific skills and experiences from both documents overlap to support the score, and identify any important job skills missing from the resume that reduce the match. Your explanation should clearly justify the score by emphasizing key matching criteria to help recruiters or systems understand the candidate’s suitability.\n",
      "\n",
      "2025/10/11 13:29:34 INFO dspy.teleprompt.mipro_optimizer_v2: 2: You are an expert recruitment analyst specializing in technical hiring. Given a job description and a candidate’s resume, carefully analyze and compare the key domain-specific and technical skills mentioned in both texts. Despite the presence of unrelated or filler content, focus on identifying relevant skills, experience, and qualifications that align between the resume and the job description. Then, assign a matching score from 0 to 10 indicating how well the candidate fits the role, considering both the coverage of required skills and any important missing qualifications. Finally, provide a clear, detailed explanation justifying the score by highlighting which critical skills were matched, which were absent, and noting the impact of any distracting or irrelevant information.\n",
      "\n",
      "2025/10/11 13:29:34 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "/Users/nhannguyen/Documents/HCMUS/LLM/Code/llm-resume-evaluation/.env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025/10/11 13:29:34 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "2025/10/11 13:29:34 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "2025/10/11 13:29:34 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 13 - Full Evaluation of Default Program ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 130.00 / 100 (130.0%): 100%|██████████| 100/100 [00:20<00:00,  4.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/11 13:29:55 INFO dspy.evaluate.evaluate: Average Metric: 130.0 / 100 (130.0%)\n",
      "2025/10/11 13:29:55 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 130.0\n",
      "\n",
      "/Users/nhannguyen/Documents/HCMUS/LLM/Code/llm-resume-evaluation/.env/lib/python3.12/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "2025/10/11 13:29:55 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 2 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 29.00 / 35 (82.9%): 100%|██████████| 35/35 [00:08<00:00,  4.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/11 13:30:03 INFO dspy.evaluate.evaluate: Average Metric: 29.0 / 35 (82.9%)\n",
      "2025/10/11 13:30:03 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 82.86 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 3'].\n",
      "2025/10/11 13:30:03 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [82.86]\n",
      "2025/10/11 13:30:03 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [130.0]\n",
      "2025/10/11 13:30:03 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 130.0\n",
      "2025/10/11 13:30:03 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/10/11 13:30:03 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 3 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 40.00 / 35 (114.3%): 100%|██████████| 35/35 [00:11<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/11 13:30:14 INFO dspy.evaluate.evaluate: Average Metric: 40.0 / 35 (114.3%)\n",
      "2025/10/11 13:30:14 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 114.29 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 0'].\n",
      "2025/10/11 13:30:14 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [82.86, 114.29]\n",
      "2025/10/11 13:30:14 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [130.0]\n",
      "2025/10/11 13:30:14 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 130.0\n",
      "2025/10/11 13:30:14 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/10/11 13:30:14 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 4 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 30.00 / 35 (85.7%): 100%|██████████| 35/35 [00:08<00:00,  4.12it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/11 13:30:23 INFO dspy.evaluate.evaluate: Average Metric: 30.0 / 35 (85.7%)\n",
      "2025/10/11 13:30:23 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 85.71 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/10/11 13:30:23 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [82.86, 114.29, 85.71]\n",
      "2025/10/11 13:30:23 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [130.0]\n",
      "2025/10/11 13:30:23 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 130.0\n",
      "2025/10/11 13:30:23 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/10/11 13:30:23 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 5 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 34.00 / 35 (97.1%): 100%|██████████| 35/35 [00:09<00:00,  3.87it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/11 13:30:32 INFO dspy.evaluate.evaluate: Average Metric: 34.0 / 35 (97.1%)\n",
      "2025/10/11 13:30:32 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 97.14 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 2'].\n",
      "2025/10/11 13:30:32 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [82.86, 114.29, 85.71, 97.14]\n",
      "2025/10/11 13:30:32 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [130.0]\n",
      "2025/10/11 13:30:32 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 130.0\n",
      "2025/10/11 13:30:32 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/10/11 13:30:32 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 6 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 31.00 / 35 (88.6%): 100%|██████████| 35/35 [00:07<00:00,  4.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/11 13:30:40 INFO dspy.evaluate.evaluate: Average Metric: 31.0 / 35 (88.6%)\n",
      "2025/10/11 13:30:40 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 88.57 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/10/11 13:30:40 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [82.86, 114.29, 85.71, 97.14, 88.57]\n",
      "2025/10/11 13:30:40 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [130.0]\n",
      "2025/10/11 13:30:40 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 130.0\n",
      "2025/10/11 13:30:40 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/10/11 13:30:40 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 7 / 13 - Full Evaluation =====\n",
      "2025/10/11 13:30:40 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 114.29) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 126.00 / 100 (126.0%): 100%|██████████| 100/100 [00:20<00:00,  4.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/11 13:31:01 INFO dspy.evaluate.evaluate: Average Metric: 126.0 / 100 (126.0%)\n",
      "2025/10/11 13:31:01 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [130.0, 126.0]\n",
      "2025/10/11 13:31:01 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 130.0\n",
      "2025/10/11 13:31:01 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/10/11 13:31:01 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/10/11 13:31:01 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 8 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 45.00 / 35 (128.6%): 100%|██████████| 35/35 [00:00<00:00, 433.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/11 13:31:01 INFO dspy.evaluate.evaluate: Average Metric: 45.0 / 35 (128.6%)\n",
      "2025/10/11 13:31:01 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 128.57 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 0'].\n",
      "2025/10/11 13:31:01 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [82.86, 114.29, 85.71, 97.14, 88.57, 128.57]\n",
      "2025/10/11 13:31:01 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [130.0, 126.0]\n",
      "2025/10/11 13:31:01 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 130.0\n",
      "2025/10/11 13:31:01 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/10/11 13:31:01 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 9 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 30.00 / 35 (85.7%): 100%|██████████| 35/35 [00:09<00:00,  3.87it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/11 13:31:10 INFO dspy.evaluate.evaluate: Average Metric: 30.0 / 35 (85.7%)\n",
      "2025/10/11 13:31:10 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 85.71 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/10/11 13:31:10 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [82.86, 114.29, 85.71, 97.14, 88.57, 128.57, 85.71]\n",
      "2025/10/11 13:31:10 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [130.0, 126.0]\n",
      "2025/10/11 13:31:10 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 130.0\n",
      "2025/10/11 13:31:10 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/10/11 13:31:10 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 10 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 38.00 / 35 (108.6%): 100%|██████████| 35/35 [00:10<00:00,  3.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/11 13:31:21 INFO dspy.evaluate.evaluate: Average Metric: 38.0 / 35 (108.6%)\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 108.57 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 4'].\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [82.86, 114.29, 85.71, 97.14, 88.57, 128.57, 85.71, 108.57]\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [130.0, 126.0]\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 130.0\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 11 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 52.00 / 35 (148.6%): 100%|██████████| 35/35 [00:00<00:00, 4929.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/11 13:31:21 INFO dspy.evaluate.evaluate: Average Metric: 52.0 / 35 (148.6%)\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 148.57 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 0'].\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [82.86, 114.29, 85.71, 97.14, 88.57, 128.57, 85.71, 108.57, 148.57]\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [130.0, 126.0]\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 130.0\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 12 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 46.00 / 35 (131.4%): 100%|██████████| 35/35 [00:00<00:00, 5090.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/11 13:31:21 INFO dspy.evaluate.evaluate: Average Metric: 46.0 / 35 (131.4%)\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 131.43 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 0'].\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [82.86, 114.29, 85.71, 97.14, 88.57, 128.57, 85.71, 108.57, 148.57, 131.43]\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [130.0, 126.0]\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 130.0\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 13 / 13 - Full Evaluation =====\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 140.0) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 130.00 / 100 (130.0%): 100%|██████████| 100/100 [00:00<00:00, 1209.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/11 13:31:21 INFO dspy.evaluate.evaluate: Average Metric: 130.0 / 100 (130.0%)\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [130.0, 126.0, 130.0]\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 130.0\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/10/11 13:31:21 INFO dspy.teleprompt.mipro_optimizer_v2: Returning best identified program with score 130.0!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "scorer = Predict(JDResumeScoring(jd, resume -> score, explanation\n",
       "    instructions='Given a job description and a resume, predict a matching score (0-10).'\n",
       "    jd = Field(annotation=str required=True json_schema_extra={'desc': 'The job description text', '__dspy_field_type': 'input', 'prefix': 'Jd:'})\n",
       "    resume = Field(annotation=str required=True json_schema_extra={'desc': \"The candidate's resume text\", '__dspy_field_type': 'input', 'prefix': 'Resume:'})\n",
       "    score = Field(annotation=str required=True json_schema_extra={'desc': 'The matching score between JD and Resume (0–10)', '__dspy_field_type': 'output', 'prefix': 'Score:'})\n",
       "    explanation = Field(annotation=str required=True json_schema_extra={'desc': 'The explanation of the matching score', '__dspy_field_type': 'output', 'prefix': 'Explanation:'})\n",
       "))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "trainset = [\n",
    "    dspy.Example(\n",
    "        jd=row[\"job_description\"], resume=row[\"resume\"], score=row[\"match_score\"]\n",
    "    ).with_inputs(\"jd\", \"resume\")\n",
    "    for i, row in train_df.iterrows()\n",
    "]\n",
    "\n",
    "valset = [\n",
    "    dspy.Example(\n",
    "        jd=row[\"job_description\"], resume=row[\"resume\"], score=row[\"match_score\"]\n",
    "    ).with_inputs(\"jd\", \"resume\")\n",
    "    for i, row in val_df.iterrows()\n",
    "]\n",
    "matcher = JDResumeMatcher()\n",
    "matcher.set_lm(llm)\n",
    "\n",
    "tuner = MIPROv2(metric=mae_metric)\n",
    "tuner.compile(matcher, trainset=trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71c72a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Optimized DSPy Prompt ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt': None,\n",
       " 'messages': [{'role': 'system',\n",
       "   'content': \"Your input fields are:\\n1. `jd` (str): The job description text\\n2. `resume` (str): The candidate's resume text\\nYour output fields are:\\n1. `score` (str): The matching score between JD and Resume (0–10)\\n2. `explanation` (str): The explanation of the matching score\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## jd ## ]]\\n{jd}\\n\\n[[ ## resume ## ]]\\n{resume}\\n\\n[[ ## score ## ]]\\n{score}\\n\\n[[ ## explanation ## ]]\\n{explanation}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Given a job description and a resume, predict a matching score (0-10).\"},\n",
       "  {'role': 'user',\n",
       "   'content': 'This is an example of the task, though some input or output fields are not supplied.\\n\\n[[ ## jd ## ]]\\nSoftware Engineer needed with experience in Git, Docker, Java, REST APIs. Hit summer discussion culture measure ever.\\n\\n[[ ## resume ## ]]\\nExperienced professional skilled in System Design, past, Java, foot. Thank case rather generation inside. Raise new structure race.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '[[ ## score ## ]]\\n4\\n\\n[[ ## explanation ## ]]\\nNot supplied for this particular example.\\n\\n[[ ## completed ## ]]\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': 'This is an example of the task, though some input or output fields are not supplied.\\n\\n[[ ## jd ## ]]\\nML Engineer needed with experience in Python, PyTorch, Computer Vision, TensorFlow, Keras. Could chair beautiful social both few through. Entire card much rate politics their identify. Pass sing goal during be those.\\n\\n[[ ## resume ## ]]\\nExperienced professional skilled in Computer Vision, Python, MLOps, PyTorch, TensorFlow, parent. Exactly section network detail. Short out team author deal hospital able.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '[[ ## score ## ]]\\n10\\n\\n[[ ## explanation ## ]]\\nNot supplied for this particular example.\\n\\n[[ ## completed ## ]]\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': 'This is an example of the task, though some input or output fields are not supplied.\\n\\n[[ ## jd ## ]]\\nML Engineer needed with experience in TensorFlow, Computer Vision, Keras, PyTorch, MLOps. Approach wish fine near. Agree long behind stuff how positive tree. Quality team general office painting official.\\n\\n[[ ## resume ## ]]\\nExperienced professional skilled in Cloud, interest, MLOps, Keras, hotel, PyTorch. Address guy fund window well impact quite. Place raise really feeling vote per. Catch board present market society fight foreign. Out generation beyond six degree stop.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '[[ ## score ## ]]\\n8\\n\\n[[ ## explanation ## ]]\\nNot supplied for this particular example.\\n\\n[[ ## completed ## ]]\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': 'This is an example of the task, though some input or output fields are not supplied.\\n\\n[[ ## jd ## ]]\\nData Scientist needed with experience in Deep Learning, Machine Learning, NLP, SQL, Pandas, Python, Statistics. Nearly computer close garden. Law individual business hair show. Final though kitchen purpose five. Use sea right civil.\\n\\n[[ ## resume ## ]]\\nExperienced professional skilled in Python, Deep Learning, garden, Statistics, SQL, Pandas, stock. Note operation despite born. Step take share million message long board.'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '[[ ## score ## ]]\\n8\\n\\n[[ ## explanation ## ]]\\nNot supplied for this particular example.\\n\\n[[ ## completed ## ]]\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': '[[ ## jd ## ]]\\nProduct Manager needed with experience in User Stories, Product Roadmap, Stakeholder Management, Agile. Yeah politics answer. View drive southern company. Notice strategy threat head grow analysis sport prepare.\\n\\n[[ ## resume ## ]]\\nExperienced professional skilled in blood, Scrum, skill, Stakeholder Management, quality, Agile. Special program two seem statement operation. Tree worry father cold daughter special.\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## score ## ]]`, then `[[ ## explanation ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}],\n",
       " 'kwargs': {},\n",
       " 'response': ModelResponse(id='chatcmpl-CPNNzulvFCZ3Qkcq6DvrFxmzVhAS3', created=1760164131, model='gpt-4.1-mini-2025-04-14', object='chat.completion', system_fingerprint='fp_c064fdde7c', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"[[ ## score ## ]]\\n7\\n\\n[[ ## explanation ## ]]\\nThe resume includes relevant skills such as Stakeholder Management and Agile, which align well with the job description's requirements. However, it lacks explicit mention of User Stories and Product Roadmap, which are key for a Product Manager role. Additionally, the resume contains some unrelated terms, which slightly lowers the match score.\\n\\n[[ ## completed ## ]]\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=79, prompt_tokens=820, total_tokens=899, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default'),\n",
       " 'outputs': [\"[[ ## score ## ]]\\n7\\n\\n[[ ## explanation ## ]]\\nThe resume includes relevant skills such as Stakeholder Management and Agile, which align well with the job description's requirements. However, it lacks explicit mention of User Stories and Product Roadmap, which are key for a Product Manager role. Additionally, the resume contains some unrelated terms, which slightly lowers the match score.\\n\\n[[ ## completed ## ]]\"],\n",
       " 'usage': {'completion_tokens': 79,\n",
       "  'prompt_tokens': 820,\n",
       "  'total_tokens': 899,\n",
       "  'completion_tokens_details': CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None),\n",
       "  'prompt_tokens_details': PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)},\n",
       " 'cost': 0.0004544,\n",
       " 'timestamp': '2025-10-11T13:28:52.103240',\n",
       " 'uuid': 'dd2b59e8-b23f-427d-846b-60d31a7de766',\n",
       " 'model': 'openai/gpt-4.1-mini-2025-04-14',\n",
       " 'response_model': 'gpt-4.1-mini-2025-04-14',\n",
       " 'model_type': 'chat'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--- Optimized DSPy Prompt ---\")\n",
    "dspy.settings.lm.history[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e5f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2023333333333333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, golds = [], []\n",
    "for example in valset:\n",
    "    result = matcher(jd=example.jd, resume=example.resume)\n",
    "    try:\n",
    "        preds.append(float(result.score))\n",
    "        golds.append(float(example.score))\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "val_mae = mean_absolute_error(golds, preds)\n",
    "val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c30f393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"preds/openai.json\", \"w\") as f:\n",
    "    json.dump(preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "556a2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.save(\"matcher/openai.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49edc4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': None,\n",
       " 'messages': [{'role': 'system',\n",
       "   'content': \"Your input fields are:\\n1. `jd` (str): The job description text\\n2. `resume` (str): The candidate's resume text\\nYour output fields are:\\n1. `score` (str): The matching score between JD and Resume (0–10)\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## jd ## ]]\\n{jd}\\n\\n[[ ## resume ## ]]\\n{resume}\\n\\n[[ ## score ## ]]\\n{score}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Given a job description and a resume, predict a matching score (0-10).\"},\n",
       "  {'role': 'user',\n",
       "   'content': '[[ ## jd ## ]]\\nProduct Manager needed with experience in Product Roadmap, Scrum, Stakeholder Management, Agile. Score around create under. Must test game quickly woman by sure business. Model along wonder drive present.\\n\\n[[ ## resume ## ]]\\nExperienced professional skilled in relationship, behind, User Stories. Tend although region product dark year. Work however tough. Window plant task list magazine trouble.\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## score ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}],\n",
       " 'kwargs': {},\n",
       " 'response': ModelResponse(id='chatcmpl-CNEFTLzTsnbGYbm4c8rC1pPVpNO1H', created=1759652351, model='gpt-4.1-mini-2025-04-14', object='chat.completion', system_fingerprint='fp_95d112f245', choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## score ## ]]\\n2\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=12, prompt_tokens=252, total_tokens=264, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default'),\n",
       " 'outputs': ['[[ ## score ## ]]\\n2\\n[[ ## completed ## ]]'],\n",
       " 'usage': {'completion_tokens': 12,\n",
       "  'prompt_tokens': 252,\n",
       "  'total_tokens': 264,\n",
       "  'completion_tokens_details': CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None),\n",
       "  'prompt_tokens_details': PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)},\n",
       " 'cost': 0.00011999999999999999,\n",
       " 'timestamp': '2025-10-05T15:19:12.269124',\n",
       " 'uuid': '3467df98-4898-41c9-9782-9097d5d71962',\n",
       " 'model': 'openai/gpt-4.1-mini-2025-04-14',\n",
       " 'response_model': 'gpt-4.1-mini-2025-04-14',\n",
       " 'model_type': 'chat'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f04fa8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-10-05T15:19:12.269124]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `jd` (str): The job description text\n",
      "2. `resume` (str): The candidate's resume text\n",
      "Your output fields are:\n",
      "1. `score` (str): The matching score between JD and Resume (0–10)\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## jd ## ]]\n",
      "{jd}\n",
      "\n",
      "[[ ## resume ## ]]\n",
      "{resume}\n",
      "\n",
      "[[ ## score ## ]]\n",
      "{score}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Given a job description and a resume, predict a matching score (0-10).\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## jd ## ]]\n",
      "Product Manager needed with experience in Product Roadmap, Scrum, Stakeholder Management, Agile. Score around create under. Must test game quickly woman by sure business. Model along wonder drive present.\n",
      "\n",
      "[[ ## resume ## ]]\n",
      "Experienced professional skilled in relationship, behind, User Stories. Tend although region product dark year. Work however tough. Window plant task list magazine trouble.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## score ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## score ## ]]\n",
      "2\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ded51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
